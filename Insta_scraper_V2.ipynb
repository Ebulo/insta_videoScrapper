{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "729a2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import config\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "833b157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the path to chromedriver.exe (download and save on your computer)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#open the webpage\n",
    "driver.get(\"https://www.instagram.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fbe88437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target username\n",
    "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='username']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='password']\")))\n",
    "\n",
    "#enter username and password\n",
    "username.clear()\n",
    "# username.send_keys(config.username)\n",
    "# Add your fake username of Insta\n",
    "username.send_keys(\"athena_ladakhi\")\n",
    "password.clear()\n",
    "# password.send_keys(config.password)\n",
    "password.send_keys(\"eco@1234\")\n",
    "\n",
    "#target the login button and click it\n",
    "button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c4813345",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "not_now_save = driver.find_element(By.XPATH, \"/html/body/div[2]/div/div/div[2]/div/div/div[1]/div[1]/div[2]/section/main/div/div/div/div/div\")\n",
    "not_now_save.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "not_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Not Now\")]')))\n",
    "\n",
    "not_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "df9ee85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait up to 10 seconds for the search button to be clickable on the web page\n",
    "search_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'svg[aria-label=\"Search\"]')))\n",
    "\n",
    "# Click the search button once it becomes clickable\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aed26dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target the search input field\n",
    "searchbox = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//input[@placeholder='Search']\")))\n",
    "searchbox.clear()\n",
    "\n",
    "#search for the @handle or keyword\n",
    "keyword = \"@jlo\"\n",
    "searchbox.send_keys(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f06c2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the keyword starts with \"@\"\n",
    "time.sleep(2)\n",
    "if keyword.startswith(\"@\"):\n",
    "    # Remove the \"@\" symbol\n",
    "    keyword = keyword[1:]\n",
    "    \n",
    "# Find the first element with the specified XPath that matches the keyword    \n",
    "first_result = driver.find_element(By.XPATH, f'//span[text()=\"{keyword}\"]')\n",
    "\n",
    "# Click on the found element (assuming it represents the desired search result)\n",
    "first_result.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1c79c06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create a BeautifulSoup object from the scraped HTML\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m soups\u001b[38;5;241m.\u001b[39mappend(\u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Get the current page height\u001b[39;00m\n\u001b[0;32m     21\u001b[0m current_height \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn document.body.scrollHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\gravityer\\scrapper\\exmp\\instagram-photo-reel-webscraping\\.venv\\Lib\\site-packages\\bs4\\__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\gravityer\\scrapper\\exmp\\instagram-photo-reel-webscraping\\.venv\\Lib\\site-packages\\bs4\\__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[1;32md:\\Projects\\gravityer\\scrapper\\exmp\\instagram-photo-reel-webscraping\\.venv\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    378\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\html\\parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\html\\parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, i):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen\u001b[38;5;241m.\u001b[39mmatch(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[0;32m    172\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\html\\parser.py:337\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cdata_mode(tag)\n",
      "File \u001b[1;32md:\\Projects\\gravityer\\scrapper\\exmp\\instagram-photo-reel-webscraping\\.venv\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:137\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[1;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m#print(\"START\", name)\u001b[39;00m\n\u001b[0;32m    136\u001b[0m sourceline, sourcepos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetpos()\n\u001b[1;32m--> 137\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourceline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourceline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43msourcepos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourcepos\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mand\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mis_empty_element \u001b[38;5;129;01mand\u001b[39;00m handle_empty_element:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# events for tags of this name.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Projects\\gravityer\\scrapper\\exmp\\instagram-photo-reel-webscraping\\.venv\\Lib\\site-packages\\bs4\\__init__.py:749\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[1;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    746\u001b[0m          \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39msearch_tag(name, attrs))):\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_classes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTag\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentTag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_most_recent_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43msourceline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourceline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourcepos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourcepos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespaces\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag\n",
      "File \u001b[1;32md:\\Projects\\gravityer\\scrapper\\exmp\\instagram-photo-reel-webscraping\\.venv\\Lib\\site-packages\\bs4\\element.py:1267\u001b[0m, in \u001b[0;36mTag.__init__\u001b[1;34m(self, parser, builder, name, namespace, prefix, attrs, parent, previous, is_xml, sourceline, sourcepos, can_be_empty_element, cdata_list_attributes, preserve_whitespace_tags, interesting_string_types, namespaces)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs)\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs)\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;66;03m# If possible, determine ahead of time whether this tag is an\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;66;03m# XML tag.\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the initial page height\n",
    "initial_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# Create a list to store htmls\n",
    "soups = []\n",
    "\n",
    "while True:\n",
    "    # Scroll down to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait for a moment to allow new content to load (adjust as needed)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Parse the HTML\n",
    "    html = driver.page_source\n",
    "    \n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    soups.append(BeautifulSoup(html, 'html.parser'))\n",
    "\n",
    "    # Get the current page height\n",
    "    current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    if current_height == initial_height:\n",
    "        break  # Exit the loop when you can't scroll further\n",
    "\n",
    "    initial_height = current_height  # Update the initial height for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa7a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 170, after: 58\n"
     ]
    }
   ],
   "source": [
    "# List to store the post image URLs\n",
    "post_urls = []\n",
    "\n",
    "for soup in soups:\n",
    "    # Find all anchor elements with href attributes\n",
    "    anchors = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Filter URLs that start with \"/p/\" or \"/reel/\"\n",
    "    post_urls.extend([anchor['href'] for anchor in anchors if anchor['href'].startswith((\"/p/\", \"/reel/\"))])\n",
    "\n",
    "# Convert the list to a set to remove duplicates\n",
    "unique_post_urls = list(set(post_urls))\n",
    "\n",
    "print(f\"before: {len(post_urls)}, after: {len(unique_post_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f4c86",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Find the <pre> tag containing the JSON data\u001b[39;00m\n\u001b[0;32m     23\u001b[0m WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[0;32m     24\u001b[0m     EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//pre\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m pre_tag \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element_by_xpath\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//pre\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Extract the JSON data from the <pre> tag\u001b[39;00m\n\u001b[0;32m     29\u001b[0m json_script \u001b[38;5;241m=\u001b[39m pre_tag\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_xpath'"
     ]
    }
   ],
   "source": [
    "json_list = []\n",
    "\n",
    "# Define the query parameters to add\n",
    "# query_parameters = \"__a=1&__d=dis\"\n",
    "query_parameters = \"__a=1&__d=dis\"\n",
    "\n",
    "# go through all urls\n",
    "for url in unique_post_urls:\n",
    "    try:\n",
    "        # Get the current URL of the page\n",
    "        current_url = driver.current_url\n",
    "\n",
    "        # Append the query parameters to the current URL\n",
    "        modified_url = \"https://www.instagram.com/\" + url + \"?\" + query_parameters\n",
    "\n",
    "        # Get URL\n",
    "        driver.get(modified_url)\n",
    "\n",
    "        # Wait for a moment to allow new content to load (adjust as needed)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Find the <pre> tag containing the JSON data\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//pre'))\n",
    "        )\n",
    "        pre_tag = driver.find_element_by_xpath('//pre')\n",
    "\n",
    "        # Extract the JSON data from the <pre> tag\n",
    "        json_script = pre_tag.text\n",
    "\n",
    "        # Parse the JSON data\n",
    "        json_parsed = json.loads(json_script)\n",
    "\n",
    "        # Add json to the list\n",
    "        json_list.append(json_parsed)\n",
    "    except (NoSuchElementException, TimeoutException, json.JSONDecodeError) as e:\n",
    "        print(\"Error Parsing\")\n",
    "        # print(f\"Error processing URL {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c032fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Lists to store URLs and corresponding dates\n",
    "all_urls = []\n",
    "all_dates = []\n",
    "\n",
    "# Iterate through each JSON data in the list\n",
    "for json_data in json_list:\n",
    "    \n",
    "    # Extract the list from the 'items' key\n",
    "    item_list = json_data.get('items', [])\n",
    "    \n",
    "    # Iterate through each item in the 'items' list\n",
    "    for item in item_list:\n",
    "        \n",
    "        # Extract the date the item was taken\n",
    "        date_taken = item.get('taken_at')  # Move this line inside the loop\n",
    "\n",
    "        # Check if 'carousel_media' is present\n",
    "        carousel_media = item.get('carousel_media', [])\n",
    "        \n",
    "        # Iterate through each media in the 'carousel_media' list\n",
    "        for media in carousel_media:\n",
    "            \n",
    "            # Extract the image URL from the media\n",
    "            image_url = media.get('image_versions2', {}).get('candidates', [{}])[0].get('url')\n",
    "            \n",
    "            if image_url:\n",
    "                # Add the image URL and corresponding date to the lists\n",
    "                all_urls.append(image_url)\n",
    "                all_dates.append(date_taken)\n",
    "                print(f\"carousel image added\")\n",
    "                \n",
    "            # Extract the video URL from the media\n",
    "            video_versions = media.get('video_versions', [])\n",
    "            if video_versions:\n",
    "                video_url = video_versions[0].get('url')\n",
    "                if video_url:\n",
    "                    \n",
    "                    # Add the video URL and corresponding date to the lists\n",
    "                    all_urls.append(video_url)\n",
    "                    all_dates.append(date_taken)\n",
    "                    print(f\"carousel video added\")\n",
    "\n",
    "        # Handle cases of unique image, instead of carousel\n",
    "        image_url = item.get('image_versions2', {}).get('candidates', [{}])[0].get('url')\n",
    "        if image_url:\n",
    "            \n",
    "            # Add the image URL and corresponding date to the lists\n",
    "            all_urls.append(image_url)\n",
    "            all_dates.append(date_taken)\n",
    "            print(f\"single image added\")\n",
    "\n",
    "        # Check if 'video_versions' key exists\n",
    "        video_versions = item.get('video_versions', [])\n",
    "        if video_versions:\n",
    "            video_url = video_versions[0].get('url')\n",
    "            if video_url:\n",
    "                all_urls.append(video_url)\n",
    "                all_dates.append(date_taken)\n",
    "                print(f\"video added\")\n",
    "                \n",
    "# Print or use all collected URLs as needed\n",
    "print(len(all_urls))\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df201782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 0 files to sample-handle\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to store downloaded files\n",
    "download_dir = keyword\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Create subfolders for images and videos\n",
    "image_dir = os.path.join(download_dir, \"images\")\n",
    "video_dir = os.path.join(download_dir, \"videos\")\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Initialize counters for images and videos\n",
    "image_counter = 1\n",
    "video_counter = 1\n",
    "\n",
    "# Iterate through URLs in the all_urls list and download media\n",
    "for index, url in enumerate(all_urls, 0):\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Extract file extension from the URL\n",
    "    url_path = urlparse(url).path\n",
    "    file_extension = os.path.splitext(url_path)[1]\n",
    "\n",
    "    # Determine the file name based on the URL\n",
    "    if file_extension.lower() in {'.jpg', '.jpeg', '.png', '.gif'}:\n",
    "        file_name = f\"{all_dates[index]}-img-{image_counter}.png\"\n",
    "        destination_folder = image_dir\n",
    "        image_counter += 1\n",
    "    elif file_extension.lower() in {'.mp4', '.avi', '.mkv', '.mov'}:\n",
    "        file_name = f\"{all_dates[index]}-vid-{video_counter}.mp4\"\n",
    "        destination_folder = video_dir\n",
    "        video_counter += 1\n",
    "    else:\n",
    "        # Default to the main download directory for other file types\n",
    "        file_name = f\"{all_dates[index]}{file_extension}\"\n",
    "        destination_folder = download_dir\n",
    "\n",
    "    # Save the file to the appropriate folder\n",
    "    file_path = os.path.join(destination_folder, file_name)\n",
    "    \n",
    "    # Write the content of the response to the file\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "    print(f\"Downloaded: {file_path}\")\n",
    "\n",
    "# Print a message indicating the number of downloaded files and the download directory\n",
    "print(f\"Downloaded {len(all_urls)} files to {download_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
